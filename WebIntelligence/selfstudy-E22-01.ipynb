{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 1\n",
    "\n",
    "Self studies should be solved individually, or in small groups of 2-3 students. There is no hand-in of your solutins to the self studies. However, you can bring your solutions to the exam, and use them as the basis for your answers to the exam questions.\n",
    "\n",
    "In this self-study we construct a simple crawler. Concretely, you should: \n",
    "\n",
    "* Select about 5 seed urls, e.g. homepages of universities, e-commerce sites, or similar\n",
    "\n",
    "* Start crawling from these seeds. Define a strategy for selecting the next url to be crawled. What kind of prioritization (if any) is embodied in your strategy?\n",
    "\n",
    "* Make sure you obey the robots.txt file, and make ensure that at least 2 seconds elapse between requests to the same host\n",
    "\n",
    "* Stop when you have crawled approx. 1000 pages\n",
    "\n",
    "* For each crawled page, save the url and the text string contained in the 'title' element of the document (we do not want to handle the full text of the pages at this point).\n",
    "\n",
    "* You can repeat this several times, using different seed sets and/or prioritization strategies.\n",
    "\n",
    "The following two self studies will extend the work that you do in this self study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following introduces a few helpful libraries and essential functions. You can use these methods, or use other tools that you are already familiar with and/or prefer to work with. \n",
    "\n",
    "A simple crawler implementation can be based on the 'requests' package [https://requests.readthedocs.io/en/master/](https://requests.readthedocs.io/en/master/) for retrieving html documents, and the BeautifulSoup parser https://www.crummy.com/software/BeautifulSoup/bs4/doc/ for parsing the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start crawling at https://www.aau.dk/ . We first retrieve the robots.txt file and check whether we are allowed to crawl the top-level url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rp=RobotFileParser()\n",
    "rp.set_url(\"https://www.aau.dk\")\n",
    "rp.read()\n",
    "print(rp.can_fetch(\"*\",\"https://www.aau.dk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the html using the requests package, which returns a response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "r=requests.get('https://www.aau.dk/')\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic view of the contents is accessible via the content attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For serious parsing, we can use the BeautifulSoup html parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_parse = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>AAU - Viden for verden - Aalborg Universitet</title>\n",
      "AAU - Viden for verden - Aalborg Universitet\n"
     ]
    }
   ],
   "source": [
    "print(r_parse.find('title'))\n",
    "print(r_parse.find('title').string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we can get all the links on the page. The following also illustrates the sleep() function to implement time delays (the following will take a while to complete; use the \"interrupt kernel\" button to terminate early):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='www.aau.dk', path='/uddannelser/optagelse/kandidat/ledige-studiepladser-2022', params='', query='', fragment='')\n",
      "https://www.aau.dk\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.aau.dk/uddannelser/optagelse/kandidat/ledige-studiepladser-2022'\n",
    "parsed = urlparse(url)\n",
    "print(parsed)\n",
    "newurl = parsed.scheme + '://' + parsed.netloc\n",
    "print(newurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tv2.dk\n",
      "1\n",
      "https://www.dr.dk\n",
      "2\n",
      "error\n",
      "https://www.aau.dk\n",
      "3\n",
      "error\n",
      "https://www.bt.dk\n",
      "4\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "seeds = ['https://www.aau.dk', 'https://www.dr.dk', 'https://www.tv2.dk', 'https://www.bt.dk']\n",
    "index_arr = {}\n",
    "crawled_links = []\n",
    "frontier = []\n",
    "frontqueue = {\n",
    "    'one' : [],\n",
    "    'two' : [],\n",
    "    'three' : []\n",
    "}\n",
    "\n",
    "back_queue = {}\n",
    "prio_heap = {}\n",
    "\n",
    "for url in seeds :\n",
    "    prio_heap[url] = datetime.now()\n",
    "    back_queue[url] = []\n",
    "sleep(2)\n",
    "\n",
    "def get_base_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    baseUrl = parsed.scheme + '://' + parsed.netloc\n",
    "    return baseUrl\n",
    "def fill_back_queue():\n",
    "    arr = []\n",
    "    if(len(frontqueue['one']) != 0):\n",
    "        arr = frontqueue['one']\n",
    "        frontqueue['one'] = []\n",
    "    elif(len(frontqueue['two']) != 0):\n",
    "        arr = frontqueue['two']\n",
    "        frontqueue['two'] = []\n",
    "    elif(len(frontqueue['three']) != 0):\n",
    "        arr = frontqueue['three']\n",
    "        frontqueue['three'] = []\n",
    "\n",
    "    for url in arr:\n",
    "        if (get_base_url(url) in back_queue.keys()):\n",
    "            back_queue[get_base_url(url)].append(url)\n",
    "            prio_heap[get_base_url(url)] = datetime.now()\n",
    "        else:\n",
    "            back_queue[get_base_url(url)] = [url]\n",
    "            prio_heap[get_base_url(url)] = datetime.now()\n",
    "\n",
    "def get_url():\n",
    "    #This should be based on a heap but :shrugeg:\n",
    "    viable_urls = [key for (key, value) in prio_heap.items() if value <= datetime.now() + timedelta(seconds=2)]\n",
    "\n",
    "    randomUrl = random.choice(viable_urls)\n",
    "    url = \"\"\n",
    "    if(len(back_queue[randomUrl]) != 0):\n",
    "        url = back_queue[randomUrl].pop()\n",
    "    else:\n",
    "        fill_back_queue()\n",
    "        return get_url()\n",
    "    crawled_links.append(randomUrl)\n",
    "    return url\n",
    "\n",
    "def fetch(url):\n",
    "    rp.set_url(get_base_url(url))\n",
    "    rp.read()\n",
    "    if (True):#rp.can_fetch(\"*\", url)):\n",
    "        r=requests.get(url)\n",
    "        r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "        return r_parse\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def index(doc, url):\n",
    "    title = doc.find('title')\n",
    "    if(title):\n",
    "        if url not in index_arr.keys():\n",
    "            index_arr[url] = title.string\n",
    "    else:\n",
    "        print('no title')\n",
    "\n",
    "def extract_urls(doc, url):\n",
    "    href_arr = [] \n",
    "    for a in doc.find_all('a', href=True):\n",
    "        link = a['href']\n",
    "        if(link.startswith('https://www') and '.dk' in link and not link.startswith('https://www.google.com')):\n",
    "            if (link not in frontier and link not in href_arr and link not in crawled_links):\n",
    "                href_arr.append(link)\n",
    "        #else:\n",
    "        #    comb_url = get_base_url(url) + link\n",
    "        #    if (comb_url not in frontier and comb_url not in href_arr and comb_url not in crawled_links):\n",
    "        #        href_arr.append(comb_url)\n",
    "    return href_arr\n",
    "\n",
    "def add_to_frontier(url_list):\n",
    "    #To make some checks easier this is added\n",
    "    for url in url_list:\n",
    "        frontier.append(url)\n",
    "        slash_count = url.count('/')\n",
    "        if (slash_count > 5):\n",
    "            frontqueue['three'].append(url)\n",
    "        elif(slash_count > 3):  \n",
    "            frontqueue['two'].append(url)\n",
    "        else:\n",
    "            frontqueue['one'].append(url)\n",
    "\n",
    "def loop_code():\n",
    "    url = get_url()\n",
    "    print(url)\n",
    "    if not url == '':\n",
    "        print(i)\n",
    "        doc = fetch(url)\n",
    "        if (doc):\n",
    "            index(doc, url)\n",
    "            add_to_frontier(extract_urls(doc, url))\n",
    "add_to_frontier(seeds)\n",
    "\n",
    "i = 0\n",
    "while (len(back_queue) != 0):\n",
    "    i += 1\n",
    "    try:\n",
    "        loop_code()\n",
    "    except:\n",
    "        print('error')\n",
    "        continue\n",
    "    if(i > 1000):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "index_frame = pd.DataFrame(index_arr, index=[0])\n",
    "index_frame.to_csv('index.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "öui\n"
     ]
    }
   ],
   "source": [
    "link = 'tv.dk'\n",
    "\n",
    "if '.dk' in link:\n",
    "    print('öui')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7198368533b4a9f7ada4a6f02ecfc216821b7313dc2a47d81fa173ab3879fbc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
